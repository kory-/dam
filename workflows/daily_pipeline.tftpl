main:
  params: [args]
  steps:
    - init:
        assign:
          # ==== Terraform から注入される定数（templatefile が展開）====
          - project_id: "${project_id}"
          - region: "${region}"
          - dataset_id: "${dataset_id}"
          - gcs_logs_bucket: "${gcs_logs_bucket}"
          - dataform_repository: "${dataform_repository}"
          - dataform_workspace: "${dataform_workspace}"

          # ==== 実行日算出（target_date 指定なければ前日） ====
          - has_target_date: '$${"target_date" in args}'
          - current_time: '$${sys.now()}'
          - today_date: '$${text.split(time.format(current_time), "T")[0]}'
          - today_timestamp: '$${time.parse(today_date + "T00:00:00Z")}'
          - yesterday_timestamp: '$${today_timestamp - 86400}'
          - default_yesterday: '$${text.substring(time.format(yesterday_timestamp), 0, 10)}'
          - processing_date: '$${if(has_target_date, map.get(args, "target_date"), default_yesterday)}'
          - dataflow_job_name: '$${"cf-logs-import-" + text.replace_all(processing_date, "-", "")}'
          - dataflow_sa: '$${"dam-dataflow-sa@" + project_id + ".iam.gserviceaccount.com"}'

    # ===================== Staging クリーンアップ（重複防止） =====================
    - cleanup_staging_before_import:
        call: http.post
        args:
          url: '$${"https://bigquery.googleapis.com/bigquery/v2/projects/" + project_id + "/jobs"}'
          auth:
            type: OAuth2
          body:
            jobReference:
              location: '$${region}'
              jobId: '$${"pre_cleanup_staging_" + text.replace_all(processing_date, "-", "") + "_" + text.replace_all(text.substring(string(sys.now()), 11, 19), ":", "")}'
            configuration:
              query:
                useLegacySql: false
                priority: "INTERACTIVE"
                parameterMode: "NAMED"
                queryParameters:
                  - name: proc_date
                    parameterType: { type: "DATE" }
                    parameterValue: { value: '$${processing_date}' }
                query: |
                  -- Dataflow実行前に対象日のデータをクリーンアップ（重複防止）
                  DELETE FROM `${project_id}.${dataset_id}.cf_logs_staging`
                  WHERE DATE(date) = @proc_date;
              labels:
                workflow: "dam-daily-pipeline"
                step: "pre_cleanup_staging"
                target_date: '$${text.replace_all(processing_date, "-", "")}'
        result: pre_cleanup_job

    - log_pre_cleanup_job:
        call: sys.log
        args:
          severity: "INFO"
          text: '$${"Pre-cleanup staging table for " + processing_date + " - Job ID: " + pre_cleanup_job.body.jobReference.jobId}'

    - wait_for_pre_cleanup:
        call: wait_for_bq_job
        args:
          project_id: '$${project_id}'
          job_id: '$${pre_cleanup_job.body.jobReference.jobId}'
          location: '$${pre_cleanup_job.body.jobReference.location}'

    # ===================== Dataflow 起動（リトライ） =====================
    - init_dataflow_retry:
        assign:
          - zones: ["a","b","c"]
          - machine_types:  ["e2-medium","e2-standard-2","n1-standard-2"]
          - dataflow_job_id: ""

    - launch_dataflow_loop:
        for:
          value: retry_attempt
          in: [0,1,2]
          steps:
            - select_zone_and_machine:
                assign:
                  - current_zone: '$${region + "-" + zones[retry_attempt % len(zones)]}'
                  - current_machine_type: '$${machine_types[retry_attempt % len(machine_types)]}'

            - log_attempt:
                call: sys.log
                args:
                  severity: "INFO"
                  text: '$${"Attempting Dataflow launch - Attempt " + string(retry_attempt + 1) + " - Zone: " + current_zone + " - Machine: " + current_machine_type}'

            - launch_dataflow:
                try:
                  steps:
                    - call_dataflow_api:
                        call: http.post
                        args:
                          url: '$${"https://dataflow.googleapis.com/v1b3/projects/" + project_id + "/locations/" + region + "/templates:launch"}'
                          query:
                            gcsPath: '$${"gs://dataflow-templates-" + region + "/latest/GCS_Text_to_BigQuery"}'
                          auth: { type: OAuth2 }
                          body:
                            jobName: '$${dataflow_job_name + "-" + string(retry_attempt)}'
                            parameters:
                              inputFilePattern: '$${"gs://" + gcs_logs_bucket + "/E3BQ0QYMSZ3FJA." + processing_date + "-*.gz"}'
                              outputTable: "${project_id}:${dataset_id}.cf_logs_staging"
                              javascriptTextTransformGcsPath: "gs://dam-dataflow-temp/scripts/transform.js"
                              javascriptTextTransformFunctionName: "transform"
                              bigQueryLoadingTemporaryDirectory: '$${"gs://dam-dataflow-temp/temp/" + processing_date}'
                              JSONPath: "gs://dam-dataflow-temp/schemas/cf_logs_schema.json"
                            environment:
                              serviceAccountEmail: '$${dataflow_sa}'
                              maxWorkers: 10
                              numWorkers: 2
                              zone: '$${current_zone}'
                              tempLocation: '$${"gs://dam-dataflow-temp/temp/" + processing_date}'
                              machineType: '$${current_machine_type}'
                        result: dataflow_response

                    - set_job_id:
                        assign:
                          - dataflow_job_id: '$${dataflow_response.body.job.id}'

                    - wait_initial:
                        call: sys.sleep
                        args: { seconds: 60 }

                    - check_job_initial_status:
                        call: http.get
                        args:
                          url: '$${"https://dataflow.googleapis.com/v1b3/projects/" + project_id + "/locations/" + region + "/jobs/" + dataflow_job_id}'
                          auth: { type: OAuth2 }
                        result: job_check

                    - evaluate_status:
                        switch:
                          - condition: '$${job_check.body.currentState == "JOB_STATE_RUNNING"}'
                            assign:
                              - launch_success: true
                            next: break
                          - condition: '$${job_check.body.currentState == "JOB_STATE_PENDING"}'
                            steps:
                              - wait_for_workers:
                                  call: sys.sleep
                                  args: { seconds: 60 }
                              - recheck_status:
                                  call: http.get
                                  args:
                                    url: '$${"https://dataflow.googleapis.com/v1b3/projects/" + project_id + "/locations/" + region + "/jobs/" + dataflow_job_id}'
                                    auth: { type: OAuth2 }
                                  result: job_recheck
                              - final_evaluate:
                                  switch:
                                    - condition: '$${job_recheck.body.currentState == "JOB_STATE_RUNNING"}'
                                      assign:
                                        - launch_success: true
                                      next: break
                                    - condition: '$${job_recheck.body.currentState == "JOB_STATE_FAILED" and retry_attempt < 2}'
                                      next: continue
                                    - condition: '$${job_recheck.body.currentState == "JOB_STATE_FAILED"}'
                                      raise:
                                        message: '$${"All Dataflow launch attempts failed. Last status: " + job_recheck.body.currentState}'
                          - condition: '$${job_check.body.currentState == "JOB_STATE_FAILED" and retry_attempt < 2}'
                            next: continue
                          - condition: '$${job_check.body.currentState == "JOB_STATE_FAILED"}'
                            raise:
                              message: '$${"All Dataflow launch attempts failed. Last status: " + job_check.body.currentState}'

                except:
                  as: e
                  steps:
                    - log_error:
                        call: sys.log
                        args:
                          severity: "WARNING"
                          text: '$${"Dataflow launch attempt " + string(retry_attempt + 1) + " failed: " + string(e)}'
                    
                    # リトライ前にstagingをクリーンアップ（重複防止）
                    - cleanup_before_retry:
                        switch:
                          - condition: '$${retry_attempt < 2}'
                            steps:
                              - log_cleanup:
                                  call: sys.log
                                  args:
                                    severity: "INFO"
                                    text: '$${"Cleaning up staging table before retry attempt " + string(retry_attempt + 2)}'
                              - cleanup_staging_for_retry:
                                  call: http.post
                                  args:
                                    url: '$${"https://bigquery.googleapis.com/bigquery/v2/projects/" + project_id + "/jobs"}'
                                    auth:
                                      type: OAuth2
                                    body:
                                      jobReference:
                                        location: '$${region}'
                                        jobId: '$${"retry_cleanup_" + text.replace_all(processing_date, "-", "") + "_" + string(retry_attempt) + "_" + text.replace_all(text.substring(string(sys.now()), 11, 19), ":", "")}'
                                      configuration:
                                        query:
                                          useLegacySql: false
                                          priority: "INTERACTIVE"
                                          parameterMode: "NAMED"
                                          queryParameters:
                                            - name: proc_date
                                              parameterType: { type: "DATE" }
                                              parameterValue: { value: '$${processing_date}' }
                                          query: |
                                            DELETE FROM `${project_id}.${dataset_id}.cf_logs_staging`
                                            WHERE DATE(date) = @proc_date;
                                  result: retry_cleanup_job
                              - wait_for_retry_cleanup:
                                  call: wait_for_bq_job
                                  args:
                                    project_id: '$${project_id}'
                                    job_id: '$${retry_cleanup_job.body.jobReference.jobId}'
                                    location: '$${retry_cleanup_job.body.jobReference.location}'
                    
                    - check_if_last_attempt:
                        switch:
                          - condition: '$${retry_attempt >= 2}'
                            raise: '$${e}'

    - check_launch_success:
        switch:
          - condition: '$${dataflow_job_id == ""}'
            raise:
              message: "Failed to launch Dataflow job after all retries"

    - log_success:
        call: sys.log
        args:
          severity: "INFO"
          text: '$${"Dataflow job launched successfully with ID: " + dataflow_job_id}'

    # ===================== Dataflow 完了待ち =====================
    - wait_for_dataflow:
        call: wait_for_job_completion
        args:
          project_id: '$${project_id}'
          region: '$${region}'
          job_id: '$${dataflow_job_id}'
        result: job_status

    - check_dataflow_status:
        switch:
          - condition: '$${job_status != "JOB_STATE_DONE"}'
            raise:
              message: '$${"Dataflow job failed with status: " + job_status}'

    # ===================== ステージング -> 本番（パーティション上書き方式） =====================
    # Step 1: 対象日のデータを削除
    - delete_partition_data:
        call: http.post
        args:
          url: '$${"https://bigquery.googleapis.com/bigquery/v2/projects/" + project_id + "/jobs"}'
          auth:
            type: OAuth2
          body:
            jobReference:
              location: '$${region}'
              jobId: '$${"delete_partition_" + text.replace_all(processing_date, "-", "") + "_" + text.replace_all(text.substring(string(sys.now()), 11, 19), ":", "")}'
            configuration:
              query:
                useLegacySql: false
                priority: "INTERACTIVE"
                parameterMode: "NAMED"
                queryParameters:
                  - name: proc_date
                    parameterType: { type: "DATE" }
                    parameterValue: { value: '$${processing_date}' }
                query: |
                  -- 対象日のパーティションを削除
                  DELETE FROM `${project_id}.${dataset_id}.cf_logs`
                  WHERE date = @proc_date;
              labels:
                workflow: "dam-daily-pipeline"
                step: "delete_partition"
                target_date: '$${text.replace_all(processing_date, "-", "")}'
        result: delete_job
    
    - log_delete_job:
        call: sys.log
        args:
          severity: "INFO"
          text: '$${"Delete partition for " + processing_date + " - Job ID: " + delete_job.body.jobReference.jobId}'
    
    - wait_for_delete_job:
        call: wait_for_bq_job
        args:
          project_id: '$${project_id}'
          job_id: '$${delete_job.body.jobReference.jobId}'
          location: '$${delete_job.body.jobReference.location}'
    
    # Step 2: 重複排除して新規データを挿入
    - insert_deduped_data:
        call: http.post
        args:
          url: '$${"https://bigquery.googleapis.com/bigquery/v2/projects/" + project_id + "/jobs"}'
          auth:
            type: OAuth2
          body:
            jobReference:
              location: '$${region}'
              jobId: '$${"insert_cf_logs_" + text.replace_all(processing_date, "-", "") + "_" + text.replace_all(text.substring(string(sys.now()), 11, 19), ":", "")}'
            configuration:
              query:
                useLegacySql: false
                priority: "INTERACTIVE"
                parameterMode: "NAMED"
                queryParameters:
                  - name: proc_date
                    parameterType: { type: "DATE" }
                    parameterValue: { value: '$${processing_date}' }
                query: |
                  -- ステージングデータから重複を排除して挿入
                  INSERT INTO `${project_id}.${dataset_id}.cf_logs`
                  WITH staged_data AS (
                    SELECT *
                    FROM `${project_id}.${dataset_id}.cf_logs_staging`
                    WHERE DATE(date) = @proc_date
                  ),
                  -- 重複排除キー生成（シンプル化）
                  keyed_data AS (
                    SELECT
                      *,
                      -- x_edge_request_idを優先、なければ複合キーを生成
                      COALESCE(
                        x_edge_request_id,
                        TO_HEX(SHA256(
                          CONCAT(
                            CAST(date AS STRING), '|',
                            IFNULL(CAST(time AS STRING), '00:00:00'), '|',
                            IFNULL(c_ip, ''), '|',
                            IFNULL(cs_method, ''), '|',
                            IFNULL(cs_host, ''), '|',
                            IFNULL(cs_uri_stem, ''), '|',
                            IFNULL(CAST(sc_status AS STRING), '0'), '|',
                            IFNULL(LOWER(cs_user_agent), ''), '|',
                            IFNULL(CAST(sc_bytes AS STRING), '0'), '|',
                            IFNULL(CAST(time_taken AS STRING), '0')
                          )
                        ))
                      ) AS _merge_key
                    FROM staged_data
                  )
                  -- 重複排除（同一キーの中で最も早い時刻のレコードを優先）
                  SELECT * EXCEPT(_merge_key, rn)
                  FROM (
                    SELECT
                      *,
                      ROW_NUMBER() OVER (
                        PARTITION BY _merge_key 
                        ORDER BY 
                          IFNULL(time, TIME '00:00:00'), 
                          x_edge_request_id NULLS LAST
                      ) AS rn
                    FROM keyed_data
                  )
                  WHERE rn = 1;
              labels:
                workflow: "dam-daily-pipeline"
                step: "insert_cf_logs"
                target_date: '$${text.replace_all(processing_date, "-", "")}'
        result: insert_job

    - log_insert_job_link:
        call: sys.log
        args:
          severity: "INFO"
          text: '$${"
BigQuery INSERT job started: https://console.cloud.google.com/bigquery?project=" + project_id + "&j=" + region + ":" + insert_job.body.jobReference.jobId + "&page=queryresults"}'

    - wait_for_insert_job:
        call: wait_for_bq_job
        args:
          project_id: '$${project_id}'
          job_id: '$${insert_job.body.jobReference.jobId}'
          location: '$${insert_job.body.jobReference.location}'

    # ステージングの対象日だけクリーンアップ（MERGE後の最終クリーンアップ）
    - cleanup_staging_for_day:
        call: http.post
        args:
          url: '$${"https://bigquery.googleapis.com/bigquery/v2/projects/" + project_id + "/jobs"}'
          auth:
            type: OAuth2
          body:
            jobReference:
              location: '$${region}'
              jobId: '$${"cleanup_staging_" + text.replace_all(processing_date, "-", "") + "_" + text.replace_all(text.substring(string(sys.now()), 11, 19), ":", "")}'
            configuration:
              query:
                useLegacySql: false
                priority: "INTERACTIVE"
                parameterMode: "NAMED"
                queryParameters:
                  - name: proc_date
                    parameterType: { type: "DATE" }
                    parameterValue: { value: '$${processing_date}' }
                query: |
                  DELETE FROM `${project_id}.${dataset_id}.cf_logs_staging`
                  WHERE DATE(date) = @proc_date;
              labels:
                workflow: "dam-daily-pipeline"
                step: "cleanup_staging"
                target_date: '$${text.replace_all(processing_date, "-", "")}'
        result: cleanup_job

    - log_cleanup_job_link:
        call: sys.log
        args:
          severity: "INFO"
          text: '$${"
BigQuery cleanup job started: https://console.cloud.google.com/bigquery?project=" + project_id + "&j=" + region + ":" + cleanup_job.body.jobReference.jobId + "&page=queryresults"}'

    - wait_for_cleanup:
        call: wait_for_bq_job
        args:
          project_id: '$${project_id}'
          job_id: '$${cleanup_job.body.jobReference.jobId}'
          location: '$${cleanup_job.body.jobReference.location}'

    # ===================== Dataform実行 =====================
    - compile_dataform:
        call: http.post
        args:
          url: '$${"https://dataform.googleapis.com/v1beta1/projects/" + project_id + "/locations/" + region + "/repositories/" + dataform_repository + "/compilationResults"}'
          auth: { type: OAuth2 }
          body:
            workspace: '$${"projects/" + project_id + "/locations/" + region + "/repositories/" + dataform_repository + "/workspaces/" + dataform_workspace}'
        result: dataform_compilation

    - wait_for_compilation:
        call: sys.sleep
        args: { seconds: 10 }

    - invoke_dataform:
        call: http.post
        args:
          url: '$${"https://dataform.googleapis.com/v1beta1/projects/" + project_id + "/locations/" + region + "/repositories/" + dataform_repository + "/workflowInvocations"}'
          auth: { type: OAuth2 }
          body:
            compilationResult: '$${dataform_compilation.body.name}'
            invocationConfig:
              includedTargets:
                - database: '$${project_id}'
                  schema: '$${dataset_id}'
                  name: "ip_features"
                - database: '$${project_id}'
                  schema: '$${dataset_id}'
                  name: "bot_ips"
                - database: '$${project_id}'
                  schema: '$${dataset_id}'
                  name: "cf_logs_filtered"
                - database: '$${project_id}'
                  schema: '$${dataset_id}'
                  name: "logs_enriched"
              transitiveDependenciesIncluded: true
              fullyRefreshIncrementalTablesEnabled: false
        result: dataform_invocation

    - wait_for_dataform:
        call: wait_for_dataform_completion
        args:
          invocation_name: '$${dataform_invocation.body.name}'
        result: dataform_status

    - check_dataform_status:
        switch:
          - condition: '$${dataform_status.state != "SUCCEEDED"}'
            raise:
              message: '$${"Dataform execution failed with state: " + dataform_status.state}'

    - return_success:
        return:
          status: "SUCCESS"
          message: '$${"Successfully processed data for " + processing_date}'
          dataflow_job_id: '$${dataflow_job_id}'
          dataform_invocation_id: '$${dataform_invocation.body.name}'

# =============== サブフロー：Dataflow 完了待ち ===============
wait_for_job_completion:
  params: [project_id, region, job_id]
  steps:
    - init_polling:
        assign:
          - max_retries: 240  # 4時間に延長（60秒 × 240 = 14400秒）
          - retry_count: 0

    - check_job:
        try:
          steps:
            - get_job_status:
                call: http.get
                args:
                  url: '$${"https://dataflow.googleapis.com/v1b3/projects/" + project_id + "/locations/" + region + "/jobs/" + job_id}'
                  auth: { type: OAuth2 }
                result: job_response
            - check_status:
                switch:
                  - condition: '$${job_response.body.currentState == "JOB_STATE_DONE"}'
                    return: "JOB_STATE_DONE"
                  - condition: '$${job_response.body.currentState == "JOB_STATE_FAILED"}'
                    return: "JOB_STATE_FAILED"
                  - condition: '$${job_response.body.currentState == "JOB_STATE_CANCELLED"}'
                    return: "JOB_STATE_CANCELLED"
                  - condition: '$${retry_count >= max_retries}'
                    return: "JOB_STATE_TIMEOUT"
            - log_progress:
                switch:
                  - condition: '$${retry_count % 10 == 0 and retry_count > 0}'
                    call: sys.log
                    args:
                      severity: "INFO"
                      text: '$${"
Dataflow job " + job_id + " still running. Waited " + string(retry_count) + " minutes..."}'
            - wait_and_retry:
                call: sys.sleep
                args: { seconds: 60 }
            - increment_retry:
                assign:
                  - retry_count: '$${retry_count + 1}'
                next: check_job
        except:
          as: e
          steps:
            - log_error:
                call: sys.log
                args:
                  severity: "ERROR"
                  data: '$${e}'
            - return_error:
                return: "JOB_STATE_ERROR"

# =============== サブフロー：Dataform 完了待ち ===============
wait_for_dataform_completion:
  params: [invocation_name]
  steps:
    - init:
        assign:
          - max_retries: 180  # 30 minutes
          - retry_count: 0
    
    - check_invocation:
        try:
          steps:
            - get_status:
                call: http.get
                args:
                  url: '$${"https://dataform.googleapis.com/v1beta1/" + invocation_name}'
                  auth: { type: OAuth2 }
                result: invocation_response
            
            - check_state:
                switch:
                  - condition: '$${invocation_response.body.state == "SUCCEEDED"}'
                    return: '$${invocation_response.body}'
                  - condition: '$${invocation_response.body.state == "FAILED"}'
                    return: '$${invocation_response.body}'
                  - condition: '$${invocation_response.body.state == "CANCELLED"}'
                    return: '$${invocation_response.body}'
                  - condition: '$${retry_count >= max_retries}'
                    raise:
                      message: "Dataform invocation timeout"
            
            - wait_and_retry:
                call: sys.sleep
                args: { seconds: 10 }
            
            - increment_retry:
                assign:
                  - retry_count: '$${retry_count + 1}'
                next: check_invocation
        except:
          as: e
          raise: '$${e}'

# =============== サブフロー：BigQuery ジョブ待ち ===============
wait_for_bq_job:
  params: [project_id, job_id, location]
  steps:
    - init:
        assign:
          - max_retries: 1080  # 3時間に延長（10秒 × 1080 = 10800秒 = 180分）
          - retry_count: 0
          - start_time: '$${sys.now()}'
          - sleep_seconds: 10
    - check_job:
        try:
          steps:
            - get_job:
                call: googleapis.bigquery.v2.jobs.get
                args:
                  projectId: '$${project_id}'
                  jobId: '$${job_id}'
                  location: '$${location}'
                result: job_result
            - check_status:
                switch:
                  - condition: '$${job_result.status.state == "DONE"}'
                    steps:
                      - check_errors:
                          switch:
                            - condition: '$${"errorResult" in job_result.status}'
                              raise:
                                message: '$${job_result.status.errorResult.message}'
                      - return_success:
                          return: "SUCCESS"
            - log_progress:
                switch:
                  - condition: '$${(retry_count * sleep_seconds) % 600 == 0 and retry_count > 0}'  # 10分ごと
                    call: sys.log
                    args:
                      severity: "INFO"
                      text: '$${"
BigQuery job " + job_id + " still running. Elapsed: " + string(retry_count * sleep_seconds / 60) + " minutes"}'
            # 長時間実行時は段階的にポーリング間隔を延長（最大30秒）
            - adjust_polling_interval:
                switch:
                  - condition: '$${retry_count > 0 and (retry_count * sleep_seconds) % 1800 == 0 and sleep_seconds < 30}'  # 30分ごとに調整
                    assign:
                      - sleep_seconds: '$${sleep_seconds + 5}'  # 5秒ずつ増やす
            - wait_and_retry:
                call: sys.sleep
                args:
                  seconds: '$${sleep_seconds}'
            - increment_retry:
                assign:
                  - retry_count: '$${retry_count + 1}'
            - check_timeout:
                switch:
                  - condition: '$${retry_count >= max_retries}'
                    raise:
                      message: '$${"
BigQuery job timeout after 180 minutes. Job ID: " + job_id}'
                next: check_job
        except:
          as: e
          raise: '$${e}'
